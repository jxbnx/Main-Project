{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25146 images belonging to 26 classes.\n",
      "Found 3958 images belonging to 26 classes.\n",
      "WARNING:tensorflow:From c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node categorical_crossentropy/softmax_cross_entropy_with_logits defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n\n  File \"C:\\Users\\jobin\\AppData\\Local\\Temp\\ipykernel_48340\\1993582026.py\", line 117, in <module>\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1807, in fit\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py\", line 5579, in categorical_crossentropy\n\nlogits and labels must be broadcastable: logits_size=[16,3] labels_size=[16,26]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_train_function_12338]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 117\u001b[0m\n\u001b[0;32m    114\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_generator,\n\u001b[0;32m    118\u001b[0m                     steps_per_epoch\u001b[38;5;241m=\u001b[39mtrain_generator\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size,\n\u001b[0;32m    119\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m    120\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39mtest_generator,\n\u001b[0;32m    121\u001b[0m                     validation_steps\u001b[38;5;241m=\u001b[39mtest_generator\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size,\n\u001b[0;32m    122\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping, reduce_lr])\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Save the classifier\u001b[39;00m\n\u001b[0;32m    127\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpotato_cnn_classifier.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node categorical_crossentropy/softmax_cross_entropy_with_logits defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n\n  File \"C:\\Users\\jobin\\AppData\\Local\\Temp\\ipykernel_48340\\1993582026.py\", line 117, in <module>\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1807, in fit\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n\n  File \"c:\\Users\\jobin\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py\", line 5579, in categorical_crossentropy\n\nlogits and labels must be broadcastable: logits_size=[16,3] labels_size=[16,26]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_train_function_12338]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, Add, Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Defining the paths to the dataset\n",
    "train_data = 'Dataset\\train'\n",
    "test_data = 'Dataset\\val'\n",
    "\n",
    "# Initializing the CNN\n",
    "np.random.seed(1337)\n",
    "\n",
    "# Defining the number of classes and other parameters\n",
    "num_classes = 3\n",
    "input_shape = (32, 32, 3)\n",
    "batch_size = 16\n",
    "epochs = 200\n",
    "\n",
    "# Normalization\n",
    "train_data_gen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=20,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Preprocess and augment the training images\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    'E:\\\\Project\\\\131223\\\\Dataset\\\\train',\n",
    "    target_size=(input_shape[0], input_shape[1]),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load and preprocess the testing dataset\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    'E:\\\\Project\\\\131223\\\\Dataset\\\\val',\n",
    "    target_size=(input_shape[0], input_shape[1]),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "\n",
    "# Define the custom ResNet-like block\n",
    "def resnet_block(x, filters, kernel_size=3, stride=1):\n",
    "    # Shortcut\n",
    "    shortcut = x\n",
    "\n",
    "    # Adjust the shortcut dimensions if needed\n",
    "    if shortcut.shape[-1] != filters or stride != 1:\n",
    "        shortcut = Conv2D(filters, kernel_size=1, strides=stride, padding='same')(shortcut)\n",
    "    \n",
    "    # First convolution layer\n",
    "    x = Conv2D(filters, kernel_size=kernel_size, strides=stride, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    # Second convolution layer\n",
    "    x = Conv2D(filters, kernel_size=kernel_size, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Add the shortcut to the main path\n",
    "    x = Add()([x, shortcut])\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "input_tensor = tf.keras.Input(shape=input_shape)\n",
    "# Build the custom ResNet-50-like model\n",
    "model = Sequential()\n",
    "\n",
    "# Initial convolution layer\n",
    "x = Conv2D(64, kernel_size=7, strides=2, padding='same', input_shape=input_shape)(input_tensor)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "\n",
    "# Residual blocks\n",
    "x = resnet_block(x, filters=64)\n",
    "x = resnet_block(x, filters=64)\n",
    "x = resnet_block(x, filters=128, stride=2)\n",
    "x = resnet_block(x, filters=128)\n",
    "x = resnet_block(x, filters=256, stride=2)\n",
    "x = resnet_block(x, filters=256)\n",
    "x = resnet_block(x, filters=128, stride=2)\n",
    "x = resnet_block(x, filters=128)\n",
    "x = resnet_block(x, filters=512, stride=2)\n",
    "x = resnet_block(x, filters=512)\n",
    "\n",
    "# Global average pooling and fully connected layer\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(units=64, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_tensor = Dense(units=num_classes, activation='softmax')(x)\n",
    "\n",
    "# Set the model's output\n",
    "model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "# Compile the classifier\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add EarlyStopping and ReduceLROnPlateau callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-8)\n",
    "\n",
    "# Train the classifier\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=train_generator.n // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=test_generator.samples // batch_size,\n",
    "                    callbacks=[early_stopping, reduce_lr]\n",
    "                    )\n",
    "\n",
    "\n",
    "# Save the classifier\n",
    "model.save('classifier.h5')\n",
    "\n",
    "# Evaluate the classifier on the testing dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "\n",
    "# Compute the predicted labels for the testing dataset\n",
    "predicted_labels = model.predict(test_generator)\n",
    "predicted_classes = np.argmax(predicted_labels, axis=1)\n",
    "\n",
    "# Get the true labels for the testing dataset\n",
    "true_labels = test_generator.classes\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy = np.sum(predicted_classes == true_labels) / len(true_labels)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_accuracy)\n",
    "print('Computed Accuracy:', accuracy)\n",
    "\n",
    "# Compute confusion matrix\n",
    "confusion = confusion_matrix(true_labels, predicted_classes)\n",
    "\n",
    "# Print confusion matrix in a good format\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "# Compute and print accuracy per class\n",
    "class_accuracy = confusion.diagonal() / confusion.sum(axis=1)\n",
    "class_names = list(test_generator.class_indices.keys())\n",
    "\n",
    "print(\"\\nAccuracy per class:\")\n",
    "for class_name, acc in zip(class_names, class_accuracy):\n",
    "    print(f\"{class_name}: {acc:.4f}\")\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "plt.plot(epochs, accuracy, 'bo-', label='Train')\n",
    "plt.plot(epochs, val_accuracy, 'ro-', label='Validation')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
